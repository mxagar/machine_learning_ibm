{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part e: CNN DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "#from tensorflow.keras.datasets import cifar10\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[444].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "print(y_train[444])\n",
    "plt.imshow(x_train[444]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding in Keras/TF\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               147968    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 11:47:06.521703: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-05 11:47:06.523236: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "# First layer: 5x5 convolution with 2x2 stride and 32 filters\n",
    "# Conv2D has these parameters:\n",
    "# - filters: the number of filters used (= depth of output)\n",
    "# - kernel_size: an (x,y) tuple giving the height and width of the kernel\n",
    "# - strides: an (x,y) tuple giving the stride in each dimension; default and common (1,1)\n",
    "# - input_shape: required only for the first layer (= image channels)\n",
    "# - padding: \"valid\" = no padding, or \"same\" = zeros evenly;\n",
    "# When padding=\"same\" and strides=1, the output has the same size as the input\n",
    "# Otherwise, general formula for the size:\n",
    "# W_out = (W_in + 2P - F)/S + 1; P: \"same\" = (F-1)/2 ?\n",
    "model_1.add(Conv2D(filters=32,\n",
    "                   kernel_size=(5, 5), # uncommon, we usually take (3,3)\n",
    "                   strides=(2,2), # uncommon, we usually take (1,1) = default\n",
    "                   padding='same', # 2, zeros\n",
    "                   input_shape=x_train.shape[1:]))\n",
    "# We can specify the activation as a layer (as done here)\n",
    "# or in the previous layer as a parameter\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "# Second layer: another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "# Third layer: 2x2 max pooling, which reduces to 3 x 3 x 32\n",
    "# Parameters od MaxPooling2D:\n",
    "# - pool_size: the (x,y) size of the grid to be pooled; 2x2 (usual) halvens the size\n",
    "# - strides: assumed to be the pool_size unless otherwise specified\n",
    "# - padding: assumed \"valid\" = no padding\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "# Flatten turns 3x3x32 into 288x1\n",
    "# Flatten appears when going from convolutional layers to\n",
    "# fully connected layers.\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(units=512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "# Always check number of paramaters!\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1.7208 - accuracy: 0.3737 - val_loss: 1.5561 - val_accuracy: 0.4413\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4411 - accuracy: 0.4779 - val_loss: 1.3076 - val_accuracy: 0.5297\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.3427 - accuracy: 0.5194 - val_loss: 1.4605 - val_accuracy: 0.5018\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2711 - accuracy: 0.5483 - val_loss: 1.1802 - val_accuracy: 0.5779\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2288 - accuracy: 0.5642 - val_loss: 1.4438 - val_accuracy: 0.5028\n",
      "Epoch 6/15\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.1913 - accuracy: 0.5773 - val_loss: 1.2276 - val_accuracy: 0.5771\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1.1610 - accuracy: 0.5905 - val_loss: 1.1163 - val_accuracy: 0.6125\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.1384 - accuracy: 0.6006 - val_loss: 1.1073 - val_accuracy: 0.6134\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1.1194 - accuracy: 0.6078 - val_loss: 1.1980 - val_accuracy: 0.5834\n",
      "Epoch 10/15\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1.1136 - accuracy: 0.6114 - val_loss: 1.0583 - val_accuracy: 0.6268\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1.1010 - accuracy: 0.6193 - val_loss: 1.0301 - val_accuracy: 0.6389\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0874 - accuracy: 0.6244 - val_loss: 1.0223 - val_accuracy: 0.6451\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.0850 - accuracy: 0.6231 - val_loss: 1.0799 - val_accuracy: 0.6411\n",
      "Epoch 14/15\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1.0818 - accuracy: 0.6285 - val_loss: 1.1641 - val_accuracy: 0.6140\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0735 - accuracy: 0.6319 - val_loss: 1.0746 - val_accuracy: 0.6302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe1411c1c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 157us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0745962530136108, 0.6302000284194946]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation loss and Validation accuracy\n",
    "model_1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6302\n"
     ]
    }
   ],
   "source": [
    "# Manual computation of the accuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_1.predict_classes(x_test)\n",
    "y_true = np.argmax(y_test, axis=1) # undo one-hot encoding\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Conv2D has these parameters:\n",
    "# - filters: the number of filters used (= depth of output)\n",
    "# - kernel_size: an (x,y) tuple giving the height and width of the kernel\n",
    "# - strides: an (x,y) tuple giving the stride in each dimension; default and common (1,1)\n",
    "# - input_shape: required only for the first layer (= image channels)\n",
    "# - padding: \"valid\" = no padding, or \"same\" = zeros evenly;\n",
    "# When padding=\"same\" and strides=1, the output has the same size as the input\n",
    "# Otherwise, general formula for the size:\n",
    "# W_out = (W_in + 2P - F)/S + 1; P: \"same\" = (F-1)/2 ?\n",
    "model_2.add(Conv2D(filters=32,\n",
    "                   kernel_size=(3,3), # common\n",
    "                   padding='same', # \n",
    "                   strides=(1,1), # common, default value\n",
    "                   input_shape=x_train.shape[1:]))\n",
    "# We can specify the activation as a layer (as done here)\n",
    "# or in the previous layer as a parameter\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "# Parameters od MaxPooling2D:\n",
    "# - pool_size: the (x,y) size of the grid to be pooled; 2x2 (usual) halvens the size\n",
    "# - strides: assumed to be the pool_size unless otherwise specified\n",
    "# - padding: assumed \"valid\" = no padding\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Flatten appears when going from convolutional layers to\n",
    "# fully connected layers.\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(units=512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(units=num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Always check number of paramaters!\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt_2 = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 336s 7ms/step - loss: 1.5668 - accuracy: 0.4352 - val_loss: 1.3224 - val_accuracy: 0.5355\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 331s 7ms/step - loss: 1.1554 - accuracy: 0.5920 - val_loss: 1.0098 - val_accuracy: 0.6436\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 329s 7ms/step - loss: 0.9914 - accuracy: 0.6545 - val_loss: 0.9027 - val_accuracy: 0.6842\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 943s 19ms/step - loss: 0.9009 - accuracy: 0.6879 - val_loss: 0.8132 - val_accuracy: 0.7226\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 746s 15ms/step - loss: 0.8427 - accuracy: 0.7085 - val_loss: 0.8109 - val_accuracy: 0.7235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe0da577050>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 18s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8109463266372681, 0.7235000133514404]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation loss and Validation accuracy\n",
    "model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
